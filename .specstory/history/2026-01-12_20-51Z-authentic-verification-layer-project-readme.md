<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 0b1b2e35-7649-4792-b242-38666a8ca716 (2026-01-12 20:51Z) -->

# Authentic Verification Layer project README (2026-01-12 20:51Z)

_**User (2026-01-12 18:53Z)**_

@src/index.ts @src/tools.ts @src/gates.ts @src/ollama.ts
Action: Write a professional `README.md`.

**Role:** Senior Open Source Maintainer.
**Tone:** Technical, concise, architectural.

**Project Title:** Authentic Verification Layer (MCP Server)
**Subtitle:** A Local-First Forensic Sidecar for AI Agents.

**Structure Requirements:**

1.  **The "Why" (Header):**
    - Explain that this is a Model Context Protocol (MCP) server designed for **local** agents (Jan, Claude Desktop).
    - Highlight the **"Eco-Epistemic Architecture"**: We solve the "Battery Drain" and "Hallucination" problems by using **Boolean Gates** (Truth Gate, Power Gate) before waking up the GPU.

2.  **Features (The Tools):**
    - List the 3 tools found in `src/tools.ts`.
    - Use their real descriptions from the code.
    - Mention that they use **Llama-3 via Ollama** for the heavy lifting.

3.  **Architecture Diagram (Mermaid):**
    - Create a text-based Mermaid diagram showing the flow:
      `User Query -> Truth/Power Gates -> (Block or Pass) -> MCP Tool -> Ollama (Localhost) -> JSON Result`.

4.  **Installation & Setup:**
    - Prerequisites: Node.js, Ollama (`llama3`).
    - Steps: `npm install`, `npm run build`.
    - **Crucial:** Explain how to configure it in **Jan Desktop** (Settings -> MCP Servers -> Command: `node`, Args: `path/to/dist/index.js`).

5.  **Troubleshooting:**
    - Mention the "No Console Log" rule (Stdio transport).
    - Mention the "Jan GPU Crash" fix (switch to CPU/Vulkan if `libcudart` fails).

6.  **License:** MIT.

**Output:** A single, formatted `README.md` file code block.

---

_**Agent (model default, mode Agent)**_



---


Reviewing the source files to understand the implementation before drafting the README.


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /home/robwistrand/authentic-mcp-server/src/index.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /home/robwistrand/authentic-mcp-server/src/tools.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /home/robwistrand/authentic-mcp-server/src/gates.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /home/robwistrand/authentic-mcp-server/src/ollama.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /home/robwistrand/authentic-mcp-server/package.json</summary>

</details>
</tool-use>

---



---


Checking for an existing README and build scripts:


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /home/robwistrand/authentic-mcp-server/README.md</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="search" data-tool-name="grep">
<details>
<summary>Tool use: **grep** • Grep for ""build"" in "/home/robwistrand/authentic-mcp-server/package.json" • 0 matches</summary>

Output mode: content


_No matches found_

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /home/robwistrand/authentic-mcp-server/tsconfig.json</summary>

</details>
</tool-use>

---



---


Creating a professional README per your requirements. Writing the file:


---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /home/robwistrand/authentic-mcp-server/README.md</summary>

**Chunk 1**
Lines added: 173, lines removed: 66

```diff
@@ -1,77 +1,184 @@
- # Authentic Verification Layer - MCP Server
- 
- A local-first Model Context Protocol (MCP) server for forensic text analysis.
- 
- ## Quick Start
+ # Authentic Verification Layer (MCP Server)
+ 
+ **A Local-First Forensic Sidecar for AI Agents**
+ 
+ ---
+ 
+ ## The "Why"
+ 
+ The Authentic Verification Layer is a Model Context Protocol (MCP) server designed for **local** AI agents (Jan Desktop, Claude Desktop, and other MCP-compatible clients). It acts as a "Forensic Sidecar," providing advanced text analysis capabilities while maintaining complete privacy and offline operation.
+ 
+ **Eco-Epistemic Architecture**: This server solves two critical problems facing local AI deployments:
+ 
+ - **Battery Drain**: Heavy GPU workloads can quickly deplete device batteries. Our "Power Gate" circuit breaker blocks oversized inputs before they reach the GPU.
+ - **Hallucination**: Unverified or unverifiable content leads to unreliable analysis. Our "Truth Gate" boolean filter blocks subjective content, gibberish, and dangerous patterns before AI processing.
+ 
+ All analysis runs through **Boolean Gates** (Truth Gate, Power Gate) that execute **before** waking up the GPU, ensuring zero wasted compute on blocked content.
+ 
+ ---
+ 
+ ## Features (The Tools)
+ 
+ The server exposes three forensic analysis tools powered by **Llama-3 via Ollama**:
+ 
+ ### 1. &#96;scan_for_fallacies&#96;
+ 
+ Analyzes text for logical fallacies and cognitive distortions. Returns a list of detected flaws with confidence ratings.
+ 
+ **Inputs:**
+ - &#96;text&#96; (string): The raw text to analyze
+ - &#96;power_mode&#96; ("eco" | "performance"): Power mode - 'eco' limits input to 2000 chars, 'performance' allows up to 10000 chars
+ 
+ ### 2. &#96;compare_sources&#96;
+ 
+ Performs a semantic "Word Diff" between two texts on the same topic. Highlights how language differs (e.g., 'Riot' vs 'Protest').
+ 
+ **Inputs:**
+ - &#96;source_a&#96; (string): Text from the first source
+ - &#96;source_b&#96; (string): Text from the second source
+ - &#96;power_mode&#96; ("eco" | "performance"): Power mode - 'eco' limits input to 2000 chars per source, 'performance' allows up to 10000 chars
+ 
+ ### 3. &#96;distill_kernel&#96;
+ 
+ Strips all adjectives, adverbs, and emotional loading from text, leaving only the verifiable subject-verb-object facts.
+ 
+ **Inputs:**
+ - &#96;text&#96; (string): The text to distill
+ - &#96;power_mode&#96; ("eco" | "performance"): Power mode - 'eco' limits input to 2000 chars, 'performance' allows up to 10000 chars
+ 
+ ---
+ 
+ ## Architecture
+ 
+ &#96;&#96;&#96;mermaid
+ flowchart TD
+     A[User Query] --&gt; B{Truth Gate}
+     B --&gt;|Block| E[Return Error]
+     B --&gt;|Pass| C{Power Gate}
+     C --&gt;|Block| E
+     C --&gt;|Pass| D[MCP Tool Handler]
+     D --&gt; F[Ollama API&lt;br/&gt;localhost:11434]
+     F --&gt; G[llama3:latest]
+     G --&gt; H[JSON Result]
+     H --&gt; I[Return to Client]
+     
+     style B fill:#ff6b6b
+     style C fill:#ffd93d
+     style F fill:#6bcf7f
+     style G fill:#6bcf7f
+ &#96;&#96;&#96;
+ 
+ **Flow:**
+ 1. **User Query** → Tool request with input text
+ 2. **Truth Gate** → Heuristic filter (blocks gibberish, spam, dangerous patterns, pure opinion)
+ 3. **Power Gate** → Circuit breaker (blocks oversized inputs in "eco" mode)
+ 4. **MCP Tool** → Routes to appropriate handler (&#96;scan_for_fallacies&#96;, &#96;compare_sources&#96;, or &#96;distill_kernel&#96;)
+ 5. **Ollama** → Local LLM API call to &#96;http://localhost:11434/api/generate&#96;
+ 6. **JSON Result** → Parsed response returned to client
+ 
+ ---
+ 
+ ## Installation &amp; Setup
  
  ### Prerequisites
  
- 1. **Node.js** (v18+)
- 2. **Ollama** running on &#96;http://localhost:11434&#96;
- 3. **Model**: &#96;llama3:latest&#96; (or other available model)
- 
- ### Installation
- 
- &#96;&#96;&#96;bash
- npm install
- npx tsc
- &#96;&#96;&#96;
+ - **Node.js** (v18 or higher)
+ - **Ollama** installed and running locally
+   - Download: https://ollama.ai
+   - Verify installation: &#96;ollama --version&#96;
+ - **Llama-3 model** pulled in Ollama:
+   &#96;&#96;&#96;bash
+   ollama pull llama3:latest
+   &#96;&#96;&#96;
+ 
+ ### Installation Steps
+ 
+ 1. **Clone the repository:**
+    &#96;&#96;&#96;bash
+    git clone &lt;repository-url&gt;
+    cd authentic-mcp-server
+    &#96;&#96;&#96;
+ 
+ 2. **Install dependencies:**
+    &#96;&#96;&#96;bash
+    npm install
+    &#96;&#96;&#96;
+ 
+ 3. **Build the TypeScript code:**
+    &#96;&#96;&#96;bash
+    npx tsc
+    &#96;&#96;&#96;
  
- ### Running the Server
+    This compiles the source files in &#96;src/&#96; to JavaScript (same directory).
  
- The MCP server uses **stdio transport** (no network port needed):
- 
- &#96;&#96;&#96;bash
- node src/index.js
- &#96;&#96;&#96;
+ 4. **Verify Ollama is running:**
+    &#96;&#96;&#96;bash
+    curl http://localhost:11434/api/tags
+    &#96;&#96;&#96;
  
- ### Configuration for Jan (MCP Host)
+ ### Configuration for Jan Desktop
  
- To use this server with Jan, configure it in Jan's MCP settings:
- 
- &#96;&#96;&#96;json
- {
-   "mcpServers": {
-     "authentic-verification-layer": {
-       "command": "node",
-       "args": ["/home/robwistrand/authentic-mcp-server/src/index.js"],
-       "env": {}
-     }
-   }
- }
- &#96;&#96;&#96;
- 
- **Important:** 
- - Replace the path with your actual absolute path
- - Use &#96;node&#96; command (not &#96;ts-node&#96;)
- - Server uses stdio transport (no port configuration needed)
- 
- ## Available Tools
- 
- 1. **scan_for_fallacies** - Analyzes text for logical fallacies
- 2. **compare_sources** - Semantic comparison between two texts
- 3. **distill_kernel** - Strips emotional language to return facts
- 
- ## Architecture
- 
- - **Transport**: Stdio (stdin/stdout)
- - **AI Backend**: Ollama (localhost:11434)
- - **Default Model**: llama3:latest
- - **Gates**: Power Gate + Truth Gate (pre-analysis filters)
+ To use this server with Jan Desktop:
+ 
+ 1. Open **Jan Desktop Settings**
+ 2. Navigate to **MCP Servers**
+ 3. Click **Add Server** or **Edit** an existing entry
+ 4. Configure as follows:
+ 
+    **Command:** &#96;node&#96;
+    
+    **Args:** &#96;/absolute/path/to/authentic-mcp-server/src/index.js&#96;
+    
+    **Example:**
+    &#96;&#96;&#96;
+    /home/robwistrand/authentic-mcp-server/src/index.js
+    &#96;&#96;&#96;
+ 
+    **Note:** Use the **absolute path** to the compiled &#96;index.js&#96; file (not &#96;index.ts&#96;).
+ 
+ 5. Restart Jan Desktop to load the server
+ 
+ The server uses **stdio transport** (standard input/output), so no port configuration is needed.
+ 
+ ---
  
  ## Troubleshooting
  
- ### Ollama Connection
- &#96;&#96;&#96;bash
- # Verify Ollama is running
- curl http://localhost:11434/api/tags
- 
- # Check available models
- ollama list
- &#96;&#96;&#96;
- 
- ### Server Issues
- - All logs go to &#96;stderr&#96; (never &#96;stdout&#96;)
- - Server must be started via stdio transport
- - Gates run before any tool execution
+ ### "No Console Log" Rule
+ 
+ **Important**: This server uses **stdio transport** for MCP communication. &#96;stdout&#96; is reserved exclusively for JSON-RPC messages. All logging, debugging, and status messages use &#96;console.error()&#96; (which writes to &#96;stderr&#96;). 
+ 
+ If you see &#96;console.log()&#96; in the code, it will break the MCP protocol. Use &#96;console.error()&#96; instead.
+ 
+ ### Jan GPU Crash Fix
+ 
+ If Jan Desktop crashes with GPU-related errors (e.g., &#96;libcudart.so&#96; not found):
+ 
+ 1. **Switch to CPU mode** in Jan Desktop settings
+ 2. **Or** configure Ollama to use CPU/Vulkan:
+    &#96;&#96;&#96;bash
+    export OLLAMA_NUM_GPU=0
+    ollama serve
+    &#96;&#96;&#96;
+ 
+ This forces Ollama to run on CPU instead of GPU, avoiding CUDA dependency issues.
+ 
+ ### Ollama Connection Issues
+ 
+ - **Connection refused**: Ensure Ollama is running (&#96;ollama serve&#96;)
+ - **Model not found**: Pull the model (&#96;ollama pull llama3:latest&#96;)
+ - **Timeout errors**: Check system resources (CPU/RAM) - Llama-3 requires significant resources
+ 
+ ### Gate Blocking Issues
+ 
+ - **Power Gate**: Input exceeds character limit for selected &#96;power_mode&#96;
+   - Solution: Use &#96;power_mode: "performance"&#96; or reduce input length
+ - **Truth Gate**: Content appears to be gibberish, spam, or pure opinion
+   - Solution: Provide substantive, verifiable content
+ 
+ ---
+ 
+ ## License
+ 
+ MIT
  
```

</details>

</tool-use>

---



---


Checking the actual gate order in the code and updating the README if needed:


---

<tool-use data-tool-type="search" data-tool-name="grep">
<details>
<summary>Tool use: **grep** • Grep for "runPowerGate|runTruthGate" in "/home/robwistrand/authentic-mcp-server/src/index.ts" • 9 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `src/index.ts` | `import \{ runPowerGate, runTruthGate \} from "./gates.js";` | L12 |
| `src/index.ts` | `const powerGateResult = runPowerGate(text, powerMode);` | L54 |
| `src/index.ts` | `const truthGateResult = runTruthGate(text);` | L60 |
| `src/index.ts` | `const powerGateA = runPowerGate(source_a, powerMode);` | L81 |
| `src/index.ts` | `const truthGateA = runTruthGate(source_a);` | L86 |
| `src/index.ts` | `const powerGateB = runPowerGate(source_b, powerMode);` | L91 |
| `src/index.ts` | `const truthGateB = runTruthGate(source_b);` | L96 |
| `src/index.ts` | `const powerGateResult = runPowerGate(text, powerMode);` | L112 |
| `src/index.ts` | `const truthGateResult = runTruthGate(text);` | L118 |


</details>
</tool-use>

---



---


The code runs Power Gate first, then Truth Gate. Updating the Mermaid diagram to match:


---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /home/robwistrand/authentic-mcp-server/README.md</summary>

**Chunk 1**
Lines added: 2, lines removed: 2

```diff
@@ -53,7 +53,7 @@
  &#96;&#96;&#96;mermaid
  flowchart TD
-     A[User Query] --&gt; B{Truth Gate}
+     A[User Query] --&gt; B{Power Gate}
      B --&gt;|Block| E[Return Error]
-     B --&gt;|Pass| C{Power Gate}
+     B --&gt;|Pass| C{Truth Gate}
      C --&gt;|Block| E
      C --&gt;|Pass| D[MCP Tool Handler]
```

**Chunk 2**
Lines added: 2, lines removed: 2

```diff
@@ -63,6 +63,6 @@
      H --&gt; I[Return to Client]
      
-     style B fill:#ff6b6b
-     style C fill:#ffd93d
+     style B fill:#ffd93d
+     style C fill:#ff6b6b
      style F fill:#6bcf7f
      style G fill:#6bcf7f
```

**Chunk 3**
Lines added: 2, lines removed: 2

```diff
@@ -71,6 +71,6 @@
  **Flow:**
  1. **User Query** → Tool request with input text
- 2. **Truth Gate** → Heuristic filter (blocks gibberish, spam, dangerous patterns, pure opinion)
- 3. **Power Gate** → Circuit breaker (blocks oversized inputs in "eco" mode)
+ 2. **Power Gate** → Circuit breaker (blocks oversized inputs in "eco" mode - fastest check)
+ 3. **Truth Gate** → Heuristic filter (blocks gibberish, spam, dangerous patterns, pure opinion)
  4. **MCP Tool** → Routes to appropriate handler (&#96;scan_for_fallacies&#96;, &#96;compare_sources&#96;, or &#96;distill_kernel&#96;)
  5. **Ollama** → Local LLM API call to &#96;http://localhost:11434/api/generate&#96;
```

</details>

</tool-use>

---



---


Created a README.md per your requirements. Summary:

✅ **The "Why" (Header)**: Explains it's an MCP server for local agents with "Eco-Epistemic Architecture" addressing battery drain and hallucination issues

✅ **Features (The Tools)**: Lists all 3 tools with exact descriptions from `src/tools.ts`:
   - `scan_for_fallacies`
   - `compare_sources`
   - `distill_kernel`

✅ **Architecture Diagram (Mermaid)**: Flowchart showing the execution flow (Power Gate → Truth Gate → MCP Tool → Ollama → JSON Result)

✅ **Installation & Setup**: 
   - Prerequisites (Node.js, Ollama, llama3)
   - Installation steps (`npm install`, `npx tsc`)
   - Jan Desktop configuration instructions

✅ **Troubleshooting**: 
   - "No Console Log" rule (stdio transport)
   - Jan GPU crash fix (CPU/Vulkan fallback)

✅ **License**: MIT

The README uses a technical, architectural tone and includes implementation details from the source code. The Mermaid diagram reflects the actual gate execution order (Power Gate first, then Truth Gate) as implemented in the code.

---

